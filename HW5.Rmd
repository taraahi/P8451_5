---
title: "Assignment 5"
date: 'February 15 2022'
output:
  html_document: default
  word_document: default
editor_options:
  chunk_output_type: console
---
#### Setup
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse) 
library(caret)
library(glmnet)
library(klaR)
```

#### Read In and Data Cleaning, data split into testing and training
```{r}
alc.data <- read.csv("./alcohol_use.csv")

str(alc.data)
summary(alc.data)
#all features are numeric except outcome variable, have to change it to a factor
#no missing codes seen

#Strip off ID Variable
alc.data$X <- NULL

#feature names are informative

#create factor for alc consumption and set reference level to not "current use"
alc.data$alc_consumption<-as.factor(alc.data$alc_consumption)
alc.data$alc_consumption<-relevel(alc.data$alc_consumption, ref = "NotCurrentUse")
#when you do classification, will focus on "current use" as target level

#partition

set.seed(100)
#make it reproducible
training.data=alc.data$alc_consumption %>% createDataPartition(p=0.7, list = F)
#creates index, output is vector with row numbers, 70/30 split
train.data=alc.data[training.data, ] #use all row numbers in training data
test.data=alc.data[-training.data, ] #use all the other row numbers
```


#Fitting the 3 models
* Model1: chooses alpha and lambda via CV using all features
* Model2: uses all features and logistic regression
* Model3: LASSO (alpha = 1, cv for lambda)

```{r}
set.seed(100)

#create user-defined vectors of lambda and alpha I'll explore using tuning
lambda=-10^seq(-3, 3, length=100)
alpha=-0.1*seq(1,10,length=10)
#will create a grid that combines 10 alpha values with 100 lambda values we're trying

#Creating model 1 using caret

model.1=train(
  alc_consumption~., data=train.data, method="glmnet", trControl=trainControl("cv", number = 10), preProcess=c("center", "scale"), tuneGrid=expand.grid(alpha=alpha, lambda=lambda)
)
#preprocess - all coefficients will be on the same scale for interpretibility of variable importance
#tuneGrid holds hyperparameters I want to try, expandgrid specifies

#output best values of alpha and lambda, different options
model.1$finalModel$tuneValue
model.1$bestTune #gives details about final model

#we can store these values
best.alpha=model.1$bestTune$alpha
best.lambda=model.1$bestTune$lambda

#to view coefficients, we have to specify value of lambda. Using specific value of lambda, code gives coefficients
#we used elastic net but every feature got shrunk to 0 except for impulsiveness score
coef(model.1$finalModel, model.1$bestTune$lambda)

#all accuracy results for different tuning parameters, this gives highest accuracy value, prints out row
model.1$results[which.max(model.1$results$Accuracy),]

#OR - if we just want accuracy 
max((model.1$results)$Accuracy)

#OR - did in class, get average accuracy (default eval metric when you have factor variable and we have classification)
confusionMatrix(model.1)

```


##Model2

```{r}
set.seed(100)

#method is changing, it's just glm and not glmnet
#still have tenfold cv
model.2=train(
  alc_consumption~., data=train.data, method="glm", trControl=trainControl("cv", number = 10), preProcess=c("center", "scale"))
#results don't have tuning of hyperparameters, we just have avg accuracy of 81%

#two ways to get eval metrics
model.2$results
confusionMatrix(model.2)

#lower than what we saw in elastic net

```

##Model 3 - LASSO
alpha = 1, still tuning lambda

```{r}
set.seed(100)

model.3=train(
  alc_consumption~., data=train.data, method="glmnet", trControl=trainControl("cv", number = 10), preProcess=c("center", "scale"), tuneGrid=expand.grid(alpha=1, lambda=lambda))
#within tuneGrid, we set alpha to 1, only tuning for lambda
#in model 1 it shrunk all but one variable, this will prob be similar

#output best values of alpha and lambda - two options
model.3$finalModel$tuneValue
model.3$bestTune
#this should be the same as model 1

confusionMatrix(model.3)

coef(model.3$finalModel, model.3$bestTune$lambda) #similar to model 1

```


So model 1 or 3 are best

##Apply to test set

```{r}
model.3.test.pred=predict(model.3, test.data) #produces predictions
#just get predictions as factors, not probabilities

#put predictions into confusion matrix function with actual observed outcomes, labeling positive as current use so it knows this is the target
confusionMatrix(model.3.test.pred, test.data$alc_consumption, positive = "CurrentUse")
```

For some reason these numbers are different than in the example

Perfect sensitivity but lower specificity. It gives PPV, NPV, etc. 



